#!/usr/bin/env python3
"""
ç®€å•çš„Tokenizeræ¼”ç¤º
"""

def manual_bpe_demo():
    """æ‰‹åŠ¨æ¼”ç¤ºBPEåŽŸç†"""
    
    print("ðŸŽ¯ BPEç®—æ³•æ‰‹åŠ¨æ¼”ç¤º")
    print("=" * 50)
    
    # å‡è®¾çš„è®­ç»ƒæ–‡æœ¬
    corpus = [
        "low lower newest widest",
        "low low low low",
        "new new new new", 
        "wide wide wide wide",
        "lowest newer widening"
    ]
    
    print("è®­ç»ƒæ–‡æœ¬:")
    for text in corpus:
        print(f"  {text}")
    
    print("\n1. åˆå§‹è¯æ±‡è¡¨ï¼ˆæ‰€æœ‰å­—ç¬¦ï¼‰:")
    initial_vocab = set()
    for text in corpus:
        for char in text.replace(" ", ""):
            initial_vocab.add(char)
    print(f"   {sorted(initial_vocab)}")
    
    print("\n2. ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘çŽ‡:")
    pairs = {}
    for text in corpus:
        words = text.split()
        for word in words:
            for i in range(len(word)-1):
                pair = (word[i], word[i+1])
                pairs[pair] = pairs.get(pair, 0) + 1
    
    # æ˜¾ç¤ºæœ€é¢‘ç¹çš„å­—ç¬¦å¯¹
    sorted_pairs = sorted(pairs.items(), key=lambda x: x[1], reverse=True)[:3]
    for (c1, c2), count in sorted_pairs:
        print(f"   '{c1}{c2}': {count}æ¬¡")
    
    print("\n3. åˆå¹¶æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹ 'lo':")
    print("   å‘çŽ° 'l' + 'o' ç»å¸¸å‡ºçŽ°ï¼Œåˆå¹¶æˆ 'lo'")
    
    print("\n4. æ–°çš„è¯æ±‡è¡¨:")
    new_vocab = initial_vocab | {'lo'}  # æ·»åŠ æ–°token
    print(f"   {sorted(new_vocab)}")
    
    print("\n5. ç”¨æ–°è¯æ±‡è¡¨é‡æ–°åˆ†è¯:")
    test_words = ["low", "lower", "newest", "widest"]
    for word in test_words:
        if word.startswith("lo"):
            segmentation = ["lo", word[2:]] if len(word) > 2 else ["lo"]
        else:
            segmentation = list(word)
        print(f"   '{word}' â†’ {segmentation}")

if __name__ == "__main__":
    manual_bpe_demo()
