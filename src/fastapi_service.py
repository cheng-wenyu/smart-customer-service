#!/usr/bin/env python3
# FastAPI AIå®¢æœæœåŠ¡

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn
import logging
import json
from datetime import datetime
import requests

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI(
    title="æ™ºèƒ½å®¢æœç³»ç»Ÿ",
    description="AIé©±åŠ¨çš„æ™ºèƒ½å®¢æœè§£å†³æ–¹æ¡ˆ",
    version="1.0.0"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•°æ®æ¨¡å‹
class ChatRequest(BaseModel):
    question: str
    session_id: str = None
    use_rag: bool = True  # æ˜¯å¦ä½¿ç”¨å‘é‡æ£€ç´¢

class ChatResponse(BaseModel):
    answer: str
    status: str
    timestamp: str
    model: str = None

# ==================== AIæ¨¡å‹é…ç½® ====================

# é€‰æ‹©ä½ çš„æ¨¡å‹ç±»å‹ï¼ˆä¿®æ”¹è¿™é‡Œï¼‰
MODEL_TYPE = "openai"  # å¯é€‰: openai, deepseek, zhipu, local

# é…ç½®ä½ çš„APIå¯†é’¥ï¼ˆè¿™é‡Œå¡«å…¥ä½ çš„å¯†é’¥ï¼‰
MODEL_CONFIG = {
    "openai": {
        "api_key": "sk-your-openai-key-here",
        "base_url": "https://api.openai.com/v1",
        "model": "gpt-3.5-turbo"
    },
    "deepseek": {
        "api_key": "sk-your-deepseek-key-here",
        "base_url": "https://api.deepseek.com",
        "model": "deepseek-chat"
    },
    # æ·»åŠ å…¶ä»–æ¨¡å‹é…ç½®
}

def call_ai_model(question: str, context: str = "") -> str:
    """
    è°ƒç”¨AIæ¨¡å‹ç”Ÿæˆå›ç­”
    
    Args:
        question: ç”¨æˆ·é—®é¢˜
        context: å¯é€‰çš„ç›¸å…³ä¸Šä¸‹æ–‡
    
    Returns:
        AIç”Ÿæˆçš„å›ç­”
    """
    try:
        if MODEL_TYPE == "openai":
            import openai
            openai.api_key = MODEL_CONFIG["openai"]["api_key"]
            openai.base_url = MODEL_CONFIG["openai"]["base_url"]
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œè¯·å‹å¥½ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"},
                {"role": "user", "content": f"{context}\n\né—®é¢˜ï¼š{question}" if context else question}
            ]
            
            response = openai.chat.completions.create(
                model=MODEL_CONFIG["openai"]["model"],
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            )
            return response.choices[0].message.content
            
        elif MODEL_TYPE == "deepseek":
            headers = {
                "Authorization": f"Bearer {MODEL_CONFIG['deepseek']['api_key']}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": MODEL_CONFIG["deepseek"]["model"],
                "messages": [
                    {"role": "user", "content": question}
                ],
                "temperature": 0.7,
                "max_tokens": 2000
            }
            
            response = requests.post(
                f"{MODEL_CONFIG['deepseek']['base_url']}/chat/completions",
                headers=headers,
                json=data,
                timeout=60
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
            
        elif MODEL_TYPE == "local":
            # å‡è®¾æœ¬åœ°æ¨¡å‹è¿è¡Œåœ¨7860ç«¯å£
            response = requests.post(
                "http://localhost:7860/chat",
                json={"question": question},
                timeout=120
            )
            return response.json().get("answer", "æœªæ”¶åˆ°å›ç­”")
            
        else:
            return f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {MODEL_TYPE}"
            
    except Exception as e:
        logger.error(f"AIæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        return f"æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„é—®é¢˜ã€‚é”™è¯¯ä¿¡æ¯ï¼š{str(e)}"

# ==================== APIç«¯ç‚¹ ====================

@app.get("/")
async def root():
    """é¦–é¡µ"""
    return {"message": "æ™ºèƒ½å®¢æœç³»ç»ŸAPIæœåŠ¡", "status": "running"}

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.post("/api/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """èŠå¤©æ¥å£"""
    try:
        logger.info(f"æ”¶åˆ°é—®é¢˜: {request.question}")
        
        # å¯é€‰ï¼šä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³çŸ¥è¯†ï¼ˆå¦‚æœå¯ç”¨RAGï¼‰
        context = ""
        if request.use_rag:
            # è¿™é‡Œå¯ä»¥æ·»åŠ å‘é‡æ•°æ®åº“æ£€ç´¢é€»è¾‘
            context = retrieve_from_vector_db(request.question)
        
        # è°ƒç”¨AIæ¨¡å‹
        answer = call_ai_model(request.question, context)
        
        logger.info(f"é—®é¢˜å¤„ç†å®Œæˆ")
        
        return ChatResponse(
            answer=answer,
            status="success",
            timestamp=datetime.now().isoformat(),
            model=MODEL_TYPE
        )
        
    except Exception as e:
        logger.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def retrieve_from_vector_db(question: str) -> str:
    """ä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
    # è¿™é‡Œå¯ä»¥é›†æˆChromaDB
    # æš‚æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²
    return ""

# ==================== å¯åŠ¨æœåŠ¡ ====================

if __name__ == "__main__":
    logger.info("ğŸŒ å¯åŠ¨WebæœåŠ¡...")
    logger.info(f"ğŸ“ æœåŠ¡åœ°å€: http://localhost:8000")
    logger.info(f"ğŸ“š APIæ–‡æ¡£: http://localhost:8000/docs")
    logger.info(f"ğŸ’¬ èŠå¤©æ¥å£: http://localhost:8000/api/chat")
    logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {MODEL_TYPE}")
    logger.info("â¹ï¸  æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("-"*50)

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
