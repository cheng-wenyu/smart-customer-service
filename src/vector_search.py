#!/usr/bin/env python3
"""
å‘é‡æœç´¢æ¨¡å— - ä½¿ç”¨ChromaDBå’ŒTransformers
"""

import os
import chromadb
import sys
import logging
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import config
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
from typing import List
from .document_processor import DocumentProcessor

#logger.debug(f"å‘é‡æœç´¢è¾“å…¥: {question}")
#logger.debug(f"å‘é‡æœç´¢è¾“å‡º: {len(results)} ä¸ªç»“æœ")


class VectorSearch:
    def __init__(self, persist_directory: str = "./chroma_db"):
        """
        åˆå§‹åŒ–å‘é‡æœç´¢ç³»ç»Ÿ
        """
        print("ğŸ”„ åˆå§‹åŒ–å‘é‡æœç´¢ç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–ChromaDBå‘é‡æ•°æ®åº“
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection = self.client.get_or_create_collection(
            name="customer_service_knowledge"
        )
        
        # åŠ è½½ä¸­æ–‡åµŒå…¥æ¨¡å‹
        print("ğŸ”„ åŠ è½½åµŒå…¥æ¨¡å‹...")
        self.tokenizer = AutoTokenizer.from_pretrained('/app/data/models/bge-small-zh')
        self.model = AutoModel.from_pretrained('/app/data/models/bge-small-zh')
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        print("âœ… å‘é‡æœç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def get_embedding(self, text: str) -> List[float]:
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡åµŒå…¥
        """
        # å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç 
        inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        
        # ç”ŸæˆåµŒå…¥
        with torch.no_grad():
            outputs = self.model(**inputs)
            # ä½¿ç”¨[CLS] tokençš„åµŒå…¥ä½œä¸ºå¥å­è¡¨ç¤º
            embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()
        
        return embedding.tolist()
    
    def add_documents(self, documents: List[str]):
        """
        å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        """
        # æ¸…ç©ºç°æœ‰é›†åˆï¼Œé¿å…é‡å¤æ·»åŠ 
        try:
            self.collection.delete(where={})
        except:
            pass
        print(f"ğŸ“ æ­£åœ¨å¤„ç† {len(documents)} ä¸ªæ–‡æ¡£...")
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£ç”ŸæˆåµŒå…¥
        embeddings = []
        for i, doc in enumerate(documents):
            if i % 10 == 0:  # æ¯10ä¸ªæ–‡æ¡£æ‰“å°ä¸€æ¬¡è¿›åº¦
                print(f"  ç”ŸæˆåµŒå…¥è¿›åº¦: {i}/{len(documents)}")
            embedding = self.get_embedding(doc)
            embeddings.append(embedding)
        
        # ä¸ºæ–‡æ¡£åˆ›å»ºID
        ids = [f"doc_{i}" for i in range(len(documents))]
        
        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        self.collection.add(
            embeddings=embeddings,
            documents=documents,
            ids=ids
        )
        
        print(f"ğŸ‰ æˆåŠŸå°† {len(documents)} ä¸ªæ–‡æ¡£æ·»åŠ åˆ°å‘é‡æ•°æ®åº“")
    
    def search(self, query: str, top_k: int = config.TOP_K_RESULTS) -> List[str]:
        """
        æœç´¢æœ€ç›¸å…³çš„æ–‡æ¡£
        """
        print(f"ğŸ” æ­£åœ¨æœç´¢: '{query}'")
        
        # å°†æŸ¥è¯¢è½¬æ¢ä¸ºåµŒå…¥
        query_embedding = self.get_embedding(query)
        
        # åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )
        
        if results['documents']:
            relevant_docs = results['documents'][0]
            print(f"âœ… æ‰¾åˆ° {len(relevant_docs)} ä¸ªç›¸å…³ç»“æœ")
            return relevant_docs
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
            return []

def demo_vector_search():
    """æ¼”ç¤ºå‘é‡æœç´¢åŠŸèƒ½"""
    print("=== å‘é‡æœç´¢æ¼”ç¤º ===\n")
    
    # 1. åŠ è½½æ–‡æ¡£
    processor = DocumentProcessor()
    documents = processor.load_documents("data/return_policy.txt")
    
    # 2. åˆå§‹åŒ–å‘é‡æœç´¢
    searcher = VectorSearch()
    
    # 3. æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
    searcher.add_documents(documents)
    
    # 4. æµ‹è¯•æœç´¢åŠŸèƒ½
    test_queries = [
        "é€€è´§éœ€è¦å‡ å¤©æ—¶é—´",
        "æ€ä¹ˆè”ç³»å®¢æœ",
        "ä»€ä¹ˆå•†å“ä¸èƒ½é€€è´§",
        "è¿è´¹è°æ‰¿æ‹…"
    ]
    
    for query in test_queries:
        print(f"\n{'='*50}")
        print(f"é—®é¢˜: {query}")
        
        results = searcher.search(query)
        
        for i, result in enumerate(results):
            print(f"\nç›¸å…³ç»“æœ {i+1}:")
            print(result)
        
        print(f"{'='*50}")

if __name__ == "__main__":
    demo_vector_search()
