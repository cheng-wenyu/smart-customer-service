#!/usr/bin/env python3
# benchmark_rag_system.py - RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·

import time
import logging
import json
import asyncio
import aiohttp
from datetime import datetime
from typing import List, Dict, Any
import statistics

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RAGBenchmark:
    """RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results = []
    
    async def test_single_query(self, session: aiohttp.ClientSession, 
                               query: str, query_id: int = None) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªæŸ¥è¯¢"""
        try:
            # å¼€å§‹è®¡æ—¶
            start_time = time.time()
            
            # å‘é€è¯·æ±‚åˆ°RAG API
            async with session.post(
                f"{self.base_url}/ask",
                json={"question": query},
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                end_time = time.time()
                
                if response.status == 200:
                    data = await response.json()
                    elapsed_time = end_time - start_time
                    
                    result = {
                        "query": query,
                        "query_id": query_id,
                        "success": True,
                        "response_time": elapsed_time,
                        "response": data.get("answer", ""),
                        "context_length": len(data.get("context", "")),
                        "timestamp": datetime.now().isoformat(),
                        "status_code": response.status
                    }
                    logger.info(f"âœ“ æŸ¥è¯¢{query_id if query_id else ''}: '{query[:30]}...' - {elapsed_time:.3f}ç§’")
                    return result
                else:
                    error_text = await response.text()
                    result = {
                        "query": query,
                        "query_id": query_id,
                        "success": False,
                        "response_time": time.time() - start_time,
                        "error": f"HTTP {response.status}: {error_text}",
                        "timestamp": datetime.now().isoformat(),
                        "status_code": response.status
                    }
                    logger.error(f"âœ— æŸ¥è¯¢{query_id if query_id else ''}: '{query[:30]}...' å¤±è´¥ - {response.status}")
                    return result
                    
        except asyncio.TimeoutError:
            result = {
                "query": query,
                "query_id": query_id,
                "success": False,
                "response_time": 30,
                "error": "è¯·æ±‚è¶…æ—¶ (30ç§’)",
                "timestamp": datetime.now().isoformat(),
                "status_code": 408
            }
            logger.error(f"âœ— æŸ¥è¯¢{query_id if query_id else ''}: '{query[:30]}...' è¶…æ—¶")
            return result
            
        except Exception as e:
            result = {
                "query": query,
                "query_id": query_id,
                "success": False,
                "response_time": time.time() - start_time,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "status_code": 500
            }
            logger.error(f"âœ— æŸ¥è¯¢{query_id if query_id else ''}: '{query[:30]}...' å¼‚å¸¸: {e}")
            return result
    
    async def run_concurrent_test(self, queries: List[str], 
                                 concurrent_requests: int = 3) -> List[Dict[str, Any]]:
        """è¿è¡Œå¹¶å‘æµ‹è¯•"""
        logger.info(f"å¼€å§‹å¹¶å‘æµ‹è¯•ï¼Œå¹¶å‘æ•°: {concurrent_requests}")
        
        # åˆ›å»ºè¿æ¥æ± 
        connector = aiohttp.TCPConnector(limit=concurrent_requests)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = []
            for i, query in enumerate(queries):
                task = asyncio.create_task(
                    self.test_single_query(session, query, i + 1)
                )
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks)
            self.results = results
            return results
    
    def generate_report(self, output_file: str = "benchmark_report.json") -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        if not self.results:
            logger.warning("æ²¡æœ‰æµ‹è¯•ç»“æœ")
            return {}
        
        # åˆ†ç¦»æˆåŠŸå’Œå¤±è´¥çš„ç»“æœ
        successful_results = [r for r in self.results if r["success"]]
        failed_results = [r for r in self.results if not r["success"]]
        
        if successful_results:
            response_times = [r["response_time"] for r in successful_results]
            context_lengths = [r["context_length"] for r in successful_results]
            
            report = {
                "test_summary": {
                    "total_queries": len(self.results),
                    "successful_queries": len(successful_results),
                    "failed_queries": len(failed_results),
                    "success_rate": len(successful_results) / len(self.results) * 100,
                    "test_timestamp": datetime.now().isoformat(),
                    "average_response_time": statistics.mean(response_times),
                    "median_response_time": statistics.median(response_times),
                    "min_response_time": min(response_times),
                    "max_response_time": max(response_times),
                    "response_time_stddev": statistics.stdev(response_times) if len(response_times) > 1 else 0,
                    "average_context_length": statistics.mean(context_lengths) if context_lengths else 0,
                    "total_test_duration": sum(response_times)
                },
                "performance_metrics": {
                    "queries_per_second": len(successful_results) / sum(response_times) if sum(response_times) > 0 else 0,
                    "average_latency": statistics.mean(response_times),
                    "p95_response_time": sorted(response_times)[int(len(response_times) * 0.95)] if response_times else 0,
                    "p99_response_time": sorted(response_times)[int(len(response_times) * 0.99)] if response_times else 0
                },
                "detailed_results": self.results,
                "failed_queries": failed_results
            }
        else:
            report = {
                "test_summary": {
                    "total_queries": len(self.results),
                    "successful_queries": 0,
                    "failed_queries": len(failed_results),
                    "success_rate": 0,
                    "test_timestamp": datetime.now().isoformat(),
                    "error": "æ‰€æœ‰æŸ¥è¯¢éƒ½å¤±è´¥äº†"
                },
                "detailed_results": self.results
            }
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°æŠ¥å‘Šæ‘˜è¦
        self.print_report_summary(report)
        
        return report
    
    def print_report_summary(self, report: Dict[str, Any]):
        """æ‰“å°æŠ¥å‘Šæ‘˜è¦"""
        print("\n" + "="*80)
        print("RAGç³»ç»Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        if "test_summary" in report:
            summary = report["test_summary"]
            print(f"\nğŸ“Š æµ‹è¯•æ¦‚è¦:")
            print(f"  æµ‹è¯•æ—¶é—´: {summary['test_timestamp']}")
            print(f"  æ€»æŸ¥è¯¢æ•°: {summary['total_queries']}")
            print(f"  æˆåŠŸæŸ¥è¯¢: {summary['successful_queries']}")
            print(f"  å¤±è´¥æŸ¥è¯¢: {summary['failed_queries']}")
            print(f"  æˆåŠŸç‡: {summary['success_rate']:.1f}%")
            
            if summary['successful_queries'] > 0:
                print(f"\nâ±ï¸ å“åº”æ—¶é—´ç»Ÿè®¡:")
                print(f"  å¹³å‡å“åº”æ—¶é—´: {summary['average_response_time']:.3f} ç§’")
                print(f"  ä¸­ä½æ•°å“åº”æ—¶é—´: {summary['median_response_time']:.3f} ç§’")
                print(f"  æœ€å°å“åº”æ—¶é—´: {summary['min_response_time']:.3f} ç§’")
                print(f"  æœ€å¤§å“åº”æ—¶é—´: {summary['max_response_time']:.3f} ç§’")
                print(f"  æ ‡å‡†å·®: {summary['response_time_stddev']:.3f} ç§’")
                
                if "performance_metrics" in report:
                    metrics = report["performance_metrics"]
                    print(f"\nğŸš€ æ€§èƒ½æŒ‡æ ‡:")
                    print(f"  æŸ¥è¯¢/ç§’: {metrics['queries_per_second']:.2f}")
                    print(f"  P95å“åº”æ—¶é—´: {metrics['p95_response_time']:.3f} ç§’")
                    print(f"  P99å“åº”æ—¶é—´: {metrics['p99_response_time']:.3f} ç§’")
        
        print("\n" + "="*80)
    
    def load_test_queries(self) -> List[str]:
        """åŠ è½½æµ‹è¯•é—®é¢˜"""
        # å°è¯•ä»æ–‡ä»¶åŠ è½½
        try:
            with open('test_questions.py', 'r', encoding='utf-8') as f:
                content = f.read()
                # ç®€å•è§£æé—®é¢˜åˆ—è¡¨
                import re
                questions = re.findall(r'question\s*=\s*["\'](.*?)["\']', content)
                if questions:
                    return questions
        except:
            pass
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é—®é¢˜
        default_queries = [
            "å¦‚ä½•é€€è´§ï¼Ÿ",
            "ä½ ä»¬çš„å”®åæœåŠ¡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ",
            "äº§å“è´¨é‡æœ‰é—®é¢˜æ€ä¹ˆåŠï¼Ÿ",
            "è®¢å•ä»€ä¹ˆæ—¶å€™å‘è´§ï¼Ÿ",
            "æ”¯æŒå“ªäº›æ”¯ä»˜æ–¹å¼ï¼Ÿ",
            "è¿è´¹æ€ä¹ˆè®¡ç®—ï¼Ÿ",
            "å¯ä»¥å¼€å‘ç¥¨å—ï¼Ÿ",
            "å•†å“æœ‰ä¿ä¿®å—ï¼Ÿ",
            "å¦‚ä½•è”ç³»å®¢æœï¼Ÿ",
            "è®¢å•èƒ½ä¿®æ”¹å—ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ ",
            "RAGæ˜¯ä»€ä¹ˆï¼Ÿ",
            "ä»€ä¹ˆæ˜¯Transformeræ¨¡å‹ï¼Ÿ",
            "å¦‚ä½•è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œï¼Ÿ"
        ]
        return default_queries

async def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸ” RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·")
    print("="*60)
    
    # è·å–æµ‹è¯•é…ç½®
    try:
        import argparse
        parser = argparse.ArgumentParser(description='RAGç³»ç»Ÿæ€§èƒ½æµ‹è¯•')
        parser.add_argument('--url', default='http://localhost:8000', help='APIæœåŠ¡åœ°å€')
        parser.add_argument('--concurrent', type=int, default=3, help='å¹¶å‘è¯·æ±‚æ•°')
        parser.add_argument('--queries', type=int, default=10, help='æµ‹è¯•é—®é¢˜æ•°é‡')
        parser.add_argument('--output', default='benchmark_report.json', help='è¾“å‡ºæ–‡ä»¶')
        args = parser.parse_args()
    except:
        # å¦‚æœargparseä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
        class Args:
            url = 'http://localhost:8000'
            concurrent = 3
            queries = 10
            output = 'benchmark_report.json'
        args = Args()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    benchmark = RAGBenchmark(base_url=args.url)
    
    # åŠ è½½æµ‹è¯•é—®é¢˜
    all_queries = benchmark.load_test_queries()
    test_queries = all_queries[:args.queries] if args.queries <= len(all_queries) else all_queries
    
    print(f"\nğŸ“‹ æµ‹è¯•é…ç½®:")
    print(f"  APIåœ°å€: {args.url}")
    print(f"  å¹¶å‘æ•°: {args.concurrent}")
    print(f"  æµ‹è¯•é—®é¢˜æ•°: {len(test_queries)}")
    print(f"  è¾“å‡ºæ–‡ä»¶: {args.output}")
    
    print("\nğŸ“ æµ‹è¯•é—®é¢˜:")
    for i, query in enumerate(test_queries, 1):
        print(f"  {i:2d}. {query}")
    
    input("\næŒ‰ Enter é”®å¼€å§‹æµ‹è¯•...")
    
    # è¿è¡Œæµ‹è¯•
    print(f"\nğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•...")
    print("-"*60)
    
    await benchmark.run_concurrent_test(test_queries, args.concurrent)
    
    # ç”ŸæˆæŠ¥å‘Š
    print(f"\nğŸ“ˆ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    benchmark.generate_report(args.output)
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")

if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())
